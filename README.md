# Eudoxia, a Data Pipeline Simulator

![Tests](https://github.com/BauplanLabs/eudoxia/workflows/Run%20Tests/badge.svg)

Eudoxia is a simulator for evaluating scheduling policies for data pipelines, executed in distributed environments.  The pipelines are represented as directed-acyclic graphs (DAGs), and node execution occurs in simulated containers.  Nodes (called "operators") in a pipeline may each be deployed to their own containers, or it is possible to batch multiple nodes together in the same container.  Either way, a node cannot execute until its parents have completed successfully.

A pipeline is considered completed when all of its operators have completed successfully.  A pipeline is never considered failed, as a scheduler may retry a failed operator.  Indeed, nodes commonly fail due to insufficient memory limits, so a scheduler may retry in a container with more memory (possibly waiting until available machines have enough memory to deploy the bigger container).  If some of the nodes in a container succeeded before a memory limit was hit, only the unfinished nodes must be retried.

Pipelines have one of three priority levels: batch (lowest), interactive, or query (highest).  A scheduler may choose to suspend running containers, to free capacity for higher priority work.  Suspended work may be redeployed again later.

## Getting Started

Make sure you have Python 3.12+ installed.  If you have (venv)[https://docs.python.org/3/tutorial/venv.html] available, you can install Eudoxia and its dependencies as follows:

```bash
# get the code
git clone https://github.com/BauplanLabs/eudoxia.git
cd eudoxia

# create a virtual environment
python3 -m venv venv
source venv/bin/activate

# install Eudoxia and its dependencies in the virtual environment
python3 -m pip install --editable .
```

You can test Eudoxia as follows:

```
pytest ./tests
```

Eudoxia has several main components: a workload, a scheduler, and an executor.  All these are configured by parameters in a single TOML file.  You can create a file with the default settings as follows:

```
eudoxia init mysim.toml
```

Feel free to edit mysim.toml.  The `scheduler_algo` field specifies which scheduling policy we will evaluate.  Other fields specify the hardware resources available, the workload characteristics, and other options.

You can run the simulator as follows, to produce performance statistics (such as pipeline latency):

```
eudoxia run mysim.toml
```

## Simulator Overview

The simulator has three main components, each implementing a `run_one_tick` method. The **workload** generates pipelines according to configured parameters. The **scheduler** decides which operators to deploy, suspend, or retryâ€”without knowing the true resource requirements. The **executor** manages physical resources (CPU and memory), deploys containers, advances running work, and reports completions and failures.

Simulation proceeds in discrete **ticks**. The tick frequency is configurable via `ticks_per_second` (default: 100,000, i.e., 10 microseconds per tick). Each tick, the workload may produce new pipelines, the scheduler issues commands, and the executor advances all running containers by one tick. Higher tick frequencies provide more precise simulation at the cost of longer execution time.

## Workload

The workload component delivers **pipelines** to the scheduler. A pipeline is a directed acyclic graph (DAG) of operators representing a user job. By default, the simulator uses a `WorkloadGenerator` that creates pipelines randomly according to configured parameters.

The key workload parameters are `waiting_seconds_mean` (average time between arrivals) and `num_pipelines` (number of pipelines per arrival). These are means; actual values are sampled from normal distributions, so arrival patterns vary naturally. The `random_seed` parameter ensures reproducibility.

Each pipeline has a **priority**: `QUERY` (highest), `INTERACTIVE`, or `BATCH_PIPELINE` (lowest). The mix is controlled by `query_prob`, `interactive_prob`, and `batch_prob`. Query pipelines are always single-operator; others have `num_operators` operators on average.

### Traces

As an alternative to random generation, workloads can be loaded from **trace files**. Traces are CSV files that specify exact pipelines, operators, and arrival times. They might be generated by external tools based on real workload observations, or captured from a random workload for reproducible testing.

Trace format (one row per operator):

| pipeline_id | arrival_seconds | priority    | operator_id | parents | resources... |
|-------------|-----------------|-------------|-------------|---------|--------------|
| p1          | 0.0             | INTERACTIVE | op1         |         |              |
| p1          |                 |             | op2         | op1     |              |
| p2          | 0.5             | QUERY       | op1         |         |              |

The `arrival_seconds` and `priority` columns are only set for the first operator of each pipeline. The `parents` column lists semicolon-separated operator IDs for DAG edges. Resource columns (`baseline_cpu_seconds`, `cpu_scaling`, `memory_gb`, `storage_read_gb`) define each operator's execution requirements.

Generate a trace from random workload parameters:

```
eudoxia gentrace mysim.toml workload.csv
```

Run a simulation with a trace (overrides workload generation parameters in the TOML):

```
eudoxia run mysim.toml -w workload.csv
```

Each operator contains one or more **segments**, which execute sequentially and represent different stages of an operator's work. Random workload generation supports multiple segments via `num_segs`, but traces currently enforce a 1:1 mapping between operators and segments.

Segments define resource usage independently for CPU and memory. The `cpu_io_ratio` parameter (0 to 1) controls the mix during random generation: higher values produce segments with longer CPU times and lower memory usage; lower values produce shorter CPU times with higher memory.

**Memory** follows one of two patterns. If `memory_gb` is set, the segment uses that fixed amount from the start. Otherwise, memory grows linearly as data is read from storage, peaking at `storage_read_gb`. The latter pattern can trigger out-of-memory failures partway through execution if the container's memory limit is exceeded.

**CPU time** is determined by `baseline_cpu_seconds`, which specifies how long a segment takes on a single core. The `cpu_scaling` function determines how runtime decreases with more cores. For example, `const` provides no parallelism benefit; `linear3` gives linear speedup up to 3 cores then remains flat; `sqrt` provides diminishing returns proportional to the square root of core count.

## Executor

The executor manages all physical resources and runs containers, typically representing a cluster. An executor manages one or more resource pools, each configured with `cpus_per_pool` CPUs and `ram_gb_per_pool` GB of memory. A resource pool is analogous to a machine: a container must run entirely within a single pool and cannot span multiple pools.

A **container** is an allocation of CPU and memory that executes one or more operators. The scheduler specifies the resources and operators for each container; the executor tracks memory usage tick by tick and reports completions and failures. If memory usage exceeds the container's allocation, an out-of-memory (OOM) error occurs and the container fails.

Containers can be **suspended** to free resources for higher priority work, but only between operator boundaries (not mid-operator). Suspension requires writing current state to disk, which takes time proportional to memory usage. Suspended operators return to pending state and can be reassigned later.

## Scheduler

The scheduler decides which operators to run, how many resources to allocate, and when to suspend work. It does not know the true resource requirements of operators; it must make decisions based on limited information and handle failures (such as OOM) by retrying with different allocations.

### Interface

Each tick, the scheduler's `run_one_tick` method receives two inputs: **results** (completions and failures from the previous tick) and **pipelines** (newly arrived work). It returns two lists: **suspensions** (containers to suspend) and **assignments** (new work to deploy).

An `Assignment` specifies a list of operators, CPU and memory allocations, priority, and target pool. A `Suspend` specifies a container ID and pool. The scheduler has access to the executor to inspect current resource availability.

### Built-in Schedulers

Three schedulers are included:

- `naive`: FIFO ordering, allocates all pool resources to one pipeline at a time, no preemption
- `priority`: serves work in priority order (query > interactive > batch), can suspend lower priority work to make room for higher priority, may starve batch work under load
- `priority-pool`: dedicates pools to priority levels (query/interactive to pool 0, batch to pool 1), requires exactly 2 pools, no preemption

### Custom Schedulers

To implement a custom scheduler, define two functions using the registration decorators:

```python
from eudoxia.scheduler.decorators import register_scheduler_init, register_scheduler

@register_scheduler_init(key="myscheduler")
def myscheduler_init(s):
    s.my_queue = []

@register_scheduler(key="myscheduler")
def myscheduler(s, results, pipelines):
    # ... scheduling logic ...
    return suspensions, assignments
```

Generate a starter scheduler with `eudoxia init`:

```
eudoxia init mysim.toml -s myscheduler
```

This creates `myscheduler.py` with a template implementation and sets `scheduler_algo = "myscheduler"` in the TOML. Run with:

```
PYTHONPATH=. eudoxia run -i myscheduler mysim.toml
```

## Programmatic Simulation

Eudoxia can be used as a Python library for scripting experiments or integrating into other tools:

```python
from eudoxia.simulator import run_simulator

# Run with a TOML file
stats = run_simulator("mysim.toml")

# Or pass parameters directly
params = {
    "duration": 300,
    "scheduler_algo": "naive",
    "num_pools": 4,
    "cpus_per_pool": 32,
    "ram_gb_per_pool": 128,
}
stats = run_simulator(params)

print(f"Pipelines completed: {stats.pipelines_all.completion_count}")
print(f"P99 latency: {stats.pipelines_all.p99_latency_seconds:.2f}s")
```

The `run_simulator` function returns a `SimulatorStats` object containing throughput, latency percentiles, failure counts, and per-priority breakdowns.

## Configuration Parameters

All parameters can be set in the TOML file. Defaults are shown below.

**Simulation**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `duration` | 600 | Simulation length in seconds |
| `ticks_per_second` | 1000 | Tick frequency (1000 = 1ms per tick) |

**Workload Generation**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `waiting_seconds_mean` | 10.0 | Mean seconds between pipeline arrivals |
| `num_pipelines` | 4 | Mean pipelines per arrival |
| `num_operators` | 5 | Mean operators per pipeline |
| `cpu_io_ratio` | 0.5 | 0=IO heavy, 1=CPU heavy |
| `interactive_prob` | 0.3 | Probability of interactive priority |
| `query_prob` | 0.1 | Probability of query priority |
| `batch_prob` | 0.6 | Probability of batch priority |
| `random_seed` | 42 | Seed for reproducibility |

**Scheduler**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `scheduler_algo` | "priority" | Scheduling algorithm to use |

**Executor**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `num_pools` | 8 | Number of resource pools |
| `cpus_per_pool` | 64 | CPUs per pool |
| `ram_gb_per_pool` | 256 | GB of RAM per pool |
| `multi_operator_containers` | true | Allow multiple operators per container |